import numpy as np
import json
import math
from datetime import datetime
from typing import Dict, List, Any
import io
import sys
from logger import setup_logger

# åˆ›å»ºæ—¥å¿—å™¨
logger = setup_logger('tennis_analyzer')

class TennisStrokeAnalyzer:
    """ç½‘çƒå‡»çƒæ£€æµ‹åˆ†æå™¨"""
    
    def __init__(self):
        self.version = "1.0.0"
    
    def analyze_stroke_from_csv_content(self, csv_content: str, threshold: float = 300.0, 
                                       slice_len: int = 200, plot: bool = False) -> Dict[str, Any]:
        """
        ä»CSVæ–‡æœ¬å†…å®¹åˆ†æç½‘çƒå‡»çƒ
        
        å‚æ•°:
            csv_content: CSVæ ¼å¼çš„æ–‡æœ¬å†…å®¹
            threshold: å‡»çƒæ£€æµ‹é˜ˆå€¼ (é»˜è®¤300)
            slice_len: å‡»çƒçª—å£é•¿åº¦ (é»˜è®¤200ä¸ªæ•°æ®ç‚¹)
            plot: æ˜¯å¦ç”Ÿæˆå›¾è¡¨ (åœ¨æœåŠ¡å™¨ä¸­é€šå¸¸è®¾ä¸ºFalse)
        
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        start_time = datetime.now()
        
        try:
            # 1. ä»CSVæ–‡æœ¬åŠ è½½æ•°æ®
            acc_data, gyro_data = self._load_csv_from_string(csv_content)
            
            if len(acc_data) == 0:
                return {
                    "success": False,
                    "error": "CSVä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®",
                    "timestamp": datetime.now().isoformat()
                }
            
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {len(acc_data)} ä¸ªæ•°æ®ç‚¹")
            
            # 2. æ£€æµ‹å‡»çƒæ—¶é—´æˆ³
            timestamps = self._detect_stroke_timestamps(gyro_data, acc_data, threshold)
            logger.info(f"ğŸ¾ åŸå§‹æ£€æµ‹åˆ° {len(timestamps)} ä¸ªå‡»çƒç‚¹")
            
            # 3. è¿‡æ»¤æ—¶é—´æˆ³ï¼ˆé¿å…é‡å¤ï¼‰
            filtered_timestamps = self._filter_timestamps(timestamps, min_gap=75)
            logger.info(f"ğŸ¾ è¿‡æ»¤åå‰©ä½™ {len(filtered_timestamps)} ä¸ªå‡»çƒç‚¹")
            
            # 4. æå–å‡»çƒçª—å£åˆ‡ç‰‡
            acc_slices, gyro_slices = self._extract_stroke_slices(
                acc_data, gyro_data, filtered_timestamps, slice_len, plot
            )
            
            # 5. åˆ†ææ¯ä¸ªå‡»çƒçš„ç‰¹å¾ï¼ŒTODOï¼šåé¢è¦æ”¹æˆç±»åˆ«/å…¶ä»–åˆ†æ
            stroke_analysis = self._analyze_strokes(acc_slices, gyro_slices)
            
            # TODO: å­˜å‚¨å‡»çƒç‰‡æ®µï¼Œä»¥ä¾¿å…¶ä»–åˆ†æ

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return {
                "success": True,
                "message": "ç½‘çƒå‡»çƒåˆ†æå®Œæˆ",
                "data": {
                    "strokes_detected": len(filtered_timestamps),
                    "timestamps": filtered_timestamps,
                    "stroke_analysis": stroke_analysis,
                    "statistics": {
                        "total_data_points": len(acc_data),
                        "stroke_rate": f"{len(filtered_timestamps)} strokes",
                        "data_duration_seconds": len(acc_data) / 5.0,  # å‡è®¾5Hzé‡‡æ ·ç‡
                        "average_interval": self._calculate_average_interval(filtered_timestamps)
                    }
                },
                "analysis_info": {
                    "method": "tennis_stroke_detection",
                    "threshold_used": threshold,
                    "window_size": slice_len,
                    "processing_time_ms": round(processing_time, 2),
                    "version": self.version
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.info(f"âŒ å‡»çƒåˆ†æé”™è¯¯: {str(e)}")
            return {
                "success": False,
                "error": f"å‡»çƒåˆ†æå¤±è´¥: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    def _load_csv_from_string(self, csv_content: str):
        """ä»å­—ç¬¦ä¸²åŠ è½½CSVæ•°æ® - é€‚é…ä½ çš„CSVæ ¼å¼"""
        lines = csv_content.strip().split('\n')
        
        logger.info(f"ğŸ“– è§£æCSVå†…å®¹ï¼Œæ€»è¡Œæ•°: {len(lines)}")
        
        if len(lines) <= 1:
            logger.warning("âš ï¸  CSVæ•°æ®ä¸è¶³ï¼ˆåªæœ‰è¡¨å¤´æˆ–æ— æ•°æ®ï¼‰")
            return np.array([]), np.array([])
        
        # æ˜¾ç¤ºè¡¨å¤´ä¿¡æ¯ç”¨äºè°ƒè¯•
        header = lines[0]
        logger.info(f"ğŸ“‹ CSVè¡¨å¤´: {header}")
        
        # è§£æè¡¨å¤´ï¼Œæ‰¾å‡ºå„åˆ—çš„ä½ç½®
        headers = [h.strip() for h in header.split(',')]
        logger.info(f"ğŸ“‹ è§£æåˆ°çš„åˆ—å: {headers}")
        logger.info(f"ğŸ“‹ åˆ—æ•°é‡: {len(headers)}")
        
        # æ‰¾å‡ºå…³é”®åˆ—çš„ä½ç½®
        column_mapping = {}
        expected_columns = ['AX', 'AY', 'AZ', 'GX', 'GY', 'GZ']
        
        for i, col in enumerate(headers):
            col_upper = col.upper()
            for expected in expected_columns:
                if expected in col_upper or col_upper in expected:
                    column_mapping[expected] = i
                    logger.info(f"ğŸ” æ‰¾åˆ°åˆ— '{col}' -> {expected} (ç´¢å¼•: {i})")
        
        logger.info(f"ğŸ“Š åˆ—æ˜ å°„ç»“æœ: {column_mapping}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['AX', 'AY', 'AZ', 'GX', 'GY', 'GZ']
        missing_cols = [col for col in required_cols if col not in column_mapping]
        
        if missing_cols:
            logger.error(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
            logger.error(f"âŒ æ‰¾åˆ°çš„åˆ—: {list(column_mapping.keys())}")
            return np.array([]), np.array([])
        
        # è·³è¿‡è¡¨å¤´ï¼Œå¼€å§‹è§£ææ•°æ®è¡Œ
        data_lines = lines[1:] if len(lines) > 1 else []
        logger.info(f"ğŸ“Š å¼€å§‹è§£æ {len(data_lines)} è¡Œæ•°æ®...")
        
        acc_data = []
        gyro_data = []
        error_count = 0
        success_count = 0
        
        # è§£æå‰å‡ è¡Œæ•°æ®ç”¨äºè°ƒè¯•
        sample_data_shown = 0
        for line_num, line in enumerate(data_lines[:5], 1):  # åªæ˜¾ç¤ºå‰5è¡Œ
            if line.strip():
                values = line.split(',')
                logger.info(f"ğŸ” ç¬¬{line_num}è¡Œç¤ºä¾‹: {values[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªå€¼
        
        # è§£ææ‰€æœ‰æ•°æ®è¡Œ
        for line_num, line in enumerate(data_lines, 1):
            if not line.strip():
                continue
                
            try:
                # åˆ†å‰²CSVè¡Œ
                values = [v.strip() for v in line.split(',')]
                
                # æå–åŠ é€Ÿåº¦æ•°æ®
                acc_x_idx = column_mapping['AX']
                acc_y_idx = column_mapping['AY']
                acc_z_idx = column_mapping['AZ']
                
                # æå–é™€èºä»ªæ•°æ®
                gyro_x_idx = column_mapping['GX']
                gyro_y_idx = column_mapping['GY']
                gyro_z_idx = column_mapping['GZ']
                
                # è§£ææ•°å€¼
                acc_x = float(values[acc_x_idx]) if acc_x_idx < len(values) else 0.0
                acc_y = float(values[acc_y_idx]) if acc_y_idx < len(values) else 0.0
                acc_z = float(values[acc_z_idx]) if acc_z_idx < len(values) else 0.0
                
                gyro_x = float(values[gyro_x_idx]) if gyro_x_idx < len(values) else 0.0
                gyro_y = float(values[gyro_y_idx]) if gyro_y_idx < len(values) else 0.0
                gyro_z = float(values[gyro_z_idx]) if gyro_z_idx < len(values) else 0.0
                
                acc_data.append([acc_x, acc_y, acc_z])
                gyro_data.append([gyro_x, gyro_y, gyro_z])
                success_count += 1
                
                # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®å€¼ç”¨äºè°ƒè¯•
                if success_count <= 3:
                    logger.info(f"âœ… ç¬¬{line_num}è¡Œæ•°æ®: "
                            f"acc=[{acc_x:.2f}, {acc_y:.2f}, {acc_z:.2f}], "
                            f"gyro=[{gyro_x:.2f}, {gyro_y:.2f}, {gyro_z:.2f}]")
                    
            except (ValueError, IndexError) as e:
                error_count += 1
                if error_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    logger.warning(f"âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {e}, æ•°æ®: {line[:50]}...")
                continue
        
        logger.info(f"ğŸ“Š è§£æå®Œæˆ: æˆåŠŸ {success_count} è¡Œ, å¤±è´¥ {error_count} è¡Œ")
        
        if success_count == 0:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ•°æ®è¡Œ")
        
        return np.array(acc_data, float), np.array(gyro_data, float)
    
    def _detect_stroke_timestamps(self, gyro, acc, threshold=300.0):
        """æ£€æµ‹å‡»çƒæ—¶é—´æˆ³"""
        gyro = np.array(gyro, float)
        acc = np.array(acc, float)
        
        # è®¡ç®—è§’é€Ÿåº¦å˜åŒ–
        gyro_diff = np.abs(np.diff(gyro, axis=0))
        
        # ç¬¦å·å˜åŒ–æ£€æµ‹
        gyro_sign_change = np.diff(np.sign(gyro), axis=0)
        acc_sign_change = np.diff(np.sign(acc), axis=0)
        
        stroke_indices = []
        
        for i in range(len(gyro_diff)):
            # æ£€æŸ¥æ˜¯å¦æœ‰è§’é€Ÿåº¦è¶…è¿‡é˜ˆå€¼
            if np.any(gyro_diff[i] > threshold):
                # æ£€æŸ¥å‰åçª—å£å†…çš„ç¬¦å·å˜åŒ–
                start = max(0, i - 3)
                end = min(len(gyro_sign_change), i + 3)
                
                has_change = (
                    np.any(np.abs(gyro_sign_change[start:end]) > 0) and
                    np.any(np.abs(acc_sign_change[start:end]) > 0)
                )
                
                if has_change:
                    stroke_indices.append(i + 1)  # +1å› ä¸ºdiffå‡å°‘äº†ç´¢å¼•
        
        return stroke_indices
    
    def _filter_timestamps(self, timestamps, min_gap=75):
        """è¿‡æ»¤æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤æ£€æµ‹"""
        if not timestamps:
            return []
        
        filtered = [timestamps[0]]
        for i in range(1, len(timestamps)):
            if timestamps[i] - timestamps[i - 1] >= min_gap:
                filtered.append(timestamps[i])
        
        return filtered
    
    def _extract_stroke_slices(self, acc, gyro, timestamps, window_size=200, plot=False):
        """æå–å‡»çƒçª—å£åˆ‡ç‰‡"""
        acc_slices = []
        gyro_slices = []
        half = window_size // 2
        
        for t in timestamps:
            start = max(t - half, 0)
            end = min(t + half, len(acc))
            
            # ç¡®ä¿çª—å£å¤§å°ä¸€è‡´
            if end - start == window_size:
                acc_slice = acc[start:end]
                gyro_slice = gyro[start:end]
                
                acc_slices.append(acc_slice.tolist())  # è½¬æ¢ä¸ºåˆ—è¡¨ä¾¿äºJSONåºåˆ—åŒ–
                gyro_slices.append(gyro_slice.tolist())
        
        return acc_slices, gyro_slices
    
    def _analyze_strokes(self, acc_slices, gyro_slices):
        """åˆ†ææ¯ä¸ªå‡»çƒçš„ç‰¹å¾"""
        if not acc_slices:
            return []
        
        stroke_analysis = []
        
        for i, (acc_slice, gyro_slice) in enumerate(zip(acc_slices, gyro_slices)):
            acc_array = np.array(acc_slice)
            gyro_array = np.array(gyro_slice)
            
            # è®¡ç®—åŸºæœ¬ç‰¹å¾
            acc_magnitude = np.sqrt(np.sum(acc_array**2, axis=1))
            gyro_magnitude = np.sqrt(np.sum(gyro_array**2, axis=1))
            
            stroke_features = {
                "stroke_id": i + 1,
                "peak_acceleration": float(np.max(acc_magnitude)),
                "peak_rotation": float(np.max(gyro_magnitude)),
                "avg_acceleration": float(np.mean(acc_magnitude)),
                "avg_rotation": float(np.mean(gyro_magnitude)),
                "stroke_power": float(np.max(acc_magnitude) * np.max(gyro_magnitude)),
                "duration_points": len(acc_slice)
            }
            
            # åˆ¤æ–­å‡»çƒç±»å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            stroke_type = self._classify_stroke_type(stroke_features)
            stroke_features["estimated_type"] = stroke_type
            
            stroke_analysis.append(stroke_features)
        
        return stroke_analysis
    
    def _classify_stroke_type(self, features):
        """æ ¹æ®ç‰¹å¾åˆ¤æ–­å‡»çƒç±»å‹"""
        peak_acc = features["peak_acceleration"]
        peak_rot = features["peak_rotation"]
        
        if peak_acc < 2.0 and peak_rot < 200:
            return "è½»å‡»/çŸ­çƒ"
        elif peak_acc < 5.0 and peak_rot < 500:
            return "æ­£å¸¸å‡»çƒ"
        elif peak_acc < 8.0:
            return "å¼ºåŠ›å‡»çƒ"
        else:
            return "éå¸¸å¼ºåŠ›å‡»çƒ"
    
    def _calculate_average_interval(self, timestamps):
        """è®¡ç®—å¹³å‡å‡»çƒé—´éš”"""
        if len(timestamps) < 2:
            return "N/A"
        
        intervals = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
        avg_interval = np.mean(intervals)
        
        # è½¬æ¢ä¸ºç§’ï¼ˆå‡è®¾5Hzé‡‡æ ·ç‡ï¼‰
        avg_seconds = avg_interval / 5.0
        return f"{avg_seconds:.1f}ç§’"
    
    def process_single_imu_csv(self, csv_path, threshold=300, slice_len=200, plot=False):
        """
        å…¼å®¹åŸå‡½æ•°çš„æ¥å£ï¼ˆä»æ–‡ä»¶è·¯å¾„è¯»å–ï¼‰
        """
        with open(csv_path, mode="r", newline="", encoding="utf-8") as f:
            csv_content = f.read()
        
        return self.analyze_stroke_from_csv_content(csv_content, threshold, slice_len, plot)

# å•ä¾‹å®ä¾‹
_stroke_analyzer = TennisStrokeAnalyzer()

# ç®€åŒ–è°ƒç”¨æ¥å£
def analyze_tennis_strokes(csv_content: str, threshold: float = 300.0, 
                          slice_len: int = 200, plot: bool = False) -> Dict[str, Any]:
    """
    ç½‘çƒå‡»çƒåˆ†æä¸»å‡½æ•°
    """
    return _stroke_analyzer.analyze_stroke_from_csv_content(csv_content, threshold, slice_len, plot)

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•CSVæ•°æ®
    test_csv = """Timestamp,DeviceName,Mac,AX,AY,AZ,GX,GY,GZ,AngX,AngY,AngZ,HX,HY,HZ,Electric,Temp
2024-01-01 10:00:00.000,Device1,AA:BB:CC:DD:EE:FF,0.1,0.2,0.9,10.2,8.3,5.1,5.2,3.1,12.5,0,0,0,100,25
2024-01-01 10:00:00.200,Device1,AA:BB:CC:DD:EE:FF,0.2,0.1,0.8,15.1,9.2,6.3,5.3,3.2,12.6,0,0,0,100,25
2024-01-01 10:00:00.400,Device1,AA:BB:CC:DD:EE:FF,0.3,0.3,1.2,350.5,280.3,310.2,5.1,3.0,12.4,0,0,0,100,25
2024-01-01 10:00:00.600,Device1,AA:BB:CC:DD:EE:FF,0.4,0.2,1.1,320.1,290.4,305.8,5.4,3.3,12.7,0,0,0,100,25
2024-01-01 10:00:00.800,Device1,AA:BB:CC:DD:EE:FF,0.2,0.3,0.9,20.3,15.2,12.1,5.0,2.9,12.3,0,0,0,100,25"""
    
    result = analyze_tennis_strokes(test_csv, threshold=300, plot=False)
    logger.info("ğŸ¾ ç½‘çƒå‡»çƒåˆ†ææµ‹è¯•ç»“æœ:")
    logger.info(json.dumps(result, indent=2, ensure_ascii=False))